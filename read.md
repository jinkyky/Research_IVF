## **ANNS**（Approximately nearest neighbours search in high dimensional spaces）背景

ANNS是指在大规模数据集中快速找到与查询点相似的最近邻，在不同领域都有重要应用的技术问题。随着数据维度升高，传统精确最近邻搜索（如线性扫描）的计算复杂度呈指数级增长，时间和内存成本无法承受。ANNS通过牺牲少量的查询精度，将复杂度从O(n)降至O(logn)乃至更低。

我们采用距离函数进行**相似度的定义**，用于量化两个对象之间的相似程度，距离函数值越小，相似度越高例如欧式距离量化数值型特征、余弦相似度量化语义方向、汉明距离量化二值化特征。ANNS一般有两种问题：1. 找出离查询点最近的K个邻居 。2. 假设查询点最近邻距离R，找出cR距离以内的所有邻居，c > 1.

以下是ANNS一些常见的应用：

**多媒体检索**：要求对包含多种类型的非结构化数据进行相似内容、关联语义的检索。通常来说，需要使用卷积神经网络提取深层语义特征，其次根据不同模态数据的特征，采用**高维向量索引技术**进行加速索引（高维特点），最后通过实时反馈（如用户跳过某些结果）调整排序，实现个性化（实时特点）。

**机器学习**：**维度灾难**现象，高维空间中数据点的距离度量失去区分能力，传统度量下所有点之间的距离趋于相似，因为所有点之间的距离值会收敛到某个固定范围，导致相似性判断失效。数学解释：

以欧氏距离为例，假设数据维度为d, 特征独立且服从相同的正态分布，对两个随机点x、y，欧氏距离平方期望为

$E(||x-y||^2) = 2d \sigma^2$

而距离平方的方差

$Var(||x-y||^2) = 4d\sigma^4$

那么相对波动性（标准差与期望比值）：

$\frac{Var}{E} = \frac{1}{\sqrt{d}}$

换句话说，所有点之间的距离会集中在某个固定值附近，**失去区分度**。

这意味着什么？这意味着依使用依赖距离度量的算法无法有效区分不同的类别，无法反映语义差异。同时，由于直接使用原始高维数据效果差，必须通过降维或特征选择压缩维度。也就是说，在ANNS中，我们**还要关注高维空间的特性：数据稀疏性和距离分布集中化，选择合适的距离度量函数和降维算法，在保留数据语义信息的前提下，重建距离的区分能力。**

**生物学和地理学：**生物和地质数据通常具有高维特征：例如蛋白质序列的嵌入表示检索蛋白质结构相似度，辅助功能预测，单细胞RNA测序数据集需要快速匹配基因片段，识别突变和进化关系，地质勘探中的地震数据需要将地震波形特征编码为高维向量，实时匹配历史事件，识别重复地震或前兆信号。

## 当前技术：

### 基于hash

**LSH（locality sensitive hashing）**：近几年比较热门的ANNS索引技术，在理论保障查询准确率的同时能实现sub-linear的查询时间。基本思想是将高维数据逐个使用随机哈希函数映射成低维表示，数据点被分配到各自的哈希桶中。LSH要求hash函数能以很大概率将相近原始高维数据尽可能的映射到相同的哈希桶中，以保证精确度和减少错误率，并要求hash函数是数据独立的。普通的LSH在大规模索引结构时难以保持高的精确度，可以采用**碰撞计数和再哈希**等技术优化性能，减小索引大小。**碰撞计数**：统计哈希表中发生碰撞的次数，用于评估哈希函数的质量或哈希表负载状态，决定是否需调整索引结构。**虚拟再哈希**：一种**渐进式再哈希**策略，将扩容过程分摊到多次操作中，避免一次性全量迁移的延迟，应用于大规模实时系统。
普通hash的基本步骤:

1. 基于量化方法将原始数据投影到码本上（二进制表示）
2. 生成l个哈希函数，对于l个哈希表
3. 对每个哈希函数$g_i$将所有数据进行投影到表$T_i$的对应桶中
4. 搜索q的k个近邻向量时对每个哈希表$T_i$计算$g_i(q)$作为索引查询桶中的向量添加到候选集S中，最后排序得到前k个近邻

缺点：难以像随机化KD树和层次化kmeans一样充分利用数据分布特点

### 基于图

**沃罗诺伊图 (Voronoi Diagram) 和德劳内图 (Delaunay Graph)**

**空圆属性 → 最近邻对属性 → Delaunay 为单调搜索网络（MSNET）**

相对邻域图（**Relative Neighborhood Graph**）RNG

NNG是MRNG（**Monotonic**），MRNG是MSNET。

MRNG，它的最大度 (Max Degree) 是一个常量，且与n无关

**HNSW（Hierarchical Navigable Small World）**：是**一种结合概率跳表（Probabilistic Skip List）分层结构和邻近图（Proximity Graph）导航机制的新型索引结构，**抛弃传统近邻图中粗搜索+近搜索的多级结构，仅用单一图结构实现全流程搜索，降低系统复杂度。基本思想是通过**多层嵌套近邻图构建层级索引，稀疏连接的高层负责快速定位，稠密连接的底层负责精细搜索。**

HNSW利用分层构建机制，将每个元素随机分配到多个层级，最高层数服从指数衰减分布，层次越高越稀疏，底层保留完整数据集。高层边（长距离连接）负责跨区域跳跃，底层边（短距离连接）负责局部精细搜索，形成**多分辨率导航网络**。每层构建NSW，采用**贪婪搜索+邻域扩展** 进行搜索，直到局部最优解，再到下一层的统一节点进入搜索。插入时，根据指数衰减概率确定节点最高层，并在每层通过启发式策略（如选择最近邻的邻接点）建立连接。由于每个节点的平均度数不超过常数，路径长度复杂度为**对数级别，故总复杂度O（logn）**

### 基于量化

量化

将多个数据量化到少个centroids的码本上

$$
x \in R^d \quad q(x) \in {c_i | i = 0,1,2...k-1}
$$

**乘积量化：Product Quantization**

**传统量化法 1.**将高维浮点向量压缩为紧凑二进制码（如64位),为保持低量化误差，需极多质心, 如64位码k = 2^64， 学习量化器需样本量远大于k，实际不可行。2. 标量量化：对每个维度独立量化，内存-误差权衡差。3. hierarchical k-means HKM分层k均值：通过树状聚类减少计算量（如先粗聚类再逐层细化），内存和训练集规模问题仍无法解决。

基本思想：1. 将数据空间划分为 k个**粗聚类**（如 k=1,000），每个聚类对应一个倒排列表（Inverted List）。2. 计算数据库向量x与其所属粗聚类中心c_i 差值 r(residual)  3. 将残差向量r分解为多个子向量（比如分为4个子向量，每段32维），再对每个子向量单独量化（如每段用256个质心）生成紧凑编码（4段×8位 = 32 位）。4. 每个粗聚类的倒排列表存的是残差的乘积量化编码，可以极大的减少内存占用。

实际中收集所有Voronoi(粗聚类)细胞的残差，统一训练一个量化器（某个子空间上），减少计算复杂度和内存需求。

### SPFresh动态更新

##### 传统方式存在的问题/挑战

1. **基于图的索引**：更新成本高（需全局调整图的边），难以支持实时更新。
2. **基于聚类的索引**：虽然更新成本低，但数据分布变化会导致分区不均衡，进而影响搜索延迟和准确率
3. **全局重建开销大**：现有系统（如DiskANN）需周期性全量重建索引，消耗大量内存（如1100GB）与计算资源（数天时间）。合并更新批次时，搜索延迟和精度波动剧烈，影响在线服务稳定性。对于聚类索引（如基于hash和基于量化）分区更新虽局部，但随数据分布漂移（如分区大小不均），索引质量（搜索精度/延迟）逐渐退化。

##### 创新点

SPFresh提出一种**基于磁盘的向量索引系统**，通过**轻量级增量再平衡协议（LIRE）实现原位更新**，无需全局重建。

1. **动态分区再平衡**：监测分区大小，及时拆分过大分区或合并过小分区，保持分区均匀。

（1）**分裂**：当分区过大时，将其均匀拆分为两个小分区

（2）**合并**：当分区过小时，将其合并到邻近分区

![](C:\Users\yycomputer\Desktop\research\photo\spfresh.png)

2. 基于**最近邻分配规则（NPA）**，仅重分配分区边界处受数据漂移影响的少量向量（通常占比极小）。避免耗时的全局重建。

3. 采用**两级流水线设计.**  一方面前台更新快速响应，记录日志并异步处理，另一方面后台多线程并行执行分区拆分、合并与向量重分配，避免阻塞查询

4. 绕过传统存储栈，直接管理NVMe SSD，优化分区追加写入。支持优先级分区读取，确保查询低延迟。



##### LIRE具体实现方式及原理

###### 基本操作

- **插入（Insert）**：将新向量追加到其最近邻分区的尾部。
- **删除（Delete）**：标记向量为“逻辑删除”（通过版本号或墓碑标记），物理删除延迟执行。
- **分裂（Split）**：当分区大小超过阈值时，将其均匀拆分为两个子分区。
- **合并（Merge）**：当分区大小低于下限时，将其合并到最近邻分区。
- **重新分配（Reassign）**：在分裂或合并后，检查邻近分区的向量是否需要调整归属。

###### 原理

1. **分裂**：分区大小超过预设上限，进行分裂。使用多约束平衡聚类算法（如SPANN中的算法）将原分区的向量均匀分为两个子分区，生成新质心；删除原分区，创建两个新分区。

必要条件：

（1）原分区向量的重新分配

若向量到旧质心的距离 ≤ 到新质心的距离，即$D(v,A_o)\le D(v,A_i), i\in \{1,2\}$，则需要检查该向量是否应归属其他邻近分区。

（2）邻近分区向量的重新分配

若邻近分区的向量到新质心的距离 ≤ 到其当前质心的距离$D(v,A_i)\le D(v,B)$，其中 B 为当前质心，则需重新分配。

**重新分配实现**

仅检查分裂分区附近的有限个邻近分区（如最近的64个）-> 对候选向量重新计算到新旧质心的距离，仅在实际违反NPA时才执行物理迁移 -> 通过递增版本号标记新分配的向量，旧版本副本在后续垃圾回收中清理



2. **合并**：分区大小低于下限，进行合并。找到最近的邻近分区，将其向量追加到该分区，接着删除原分区的质心。

**重新分配实现**

被合并分区的所有向量需重新检查是否满足NPA属性

## 主要研究方向

### 加强降维处理

常见降维方法有PCA，MDS，Sammon  mapping， t-SNE， Isoma，LargeVis。前三者试图保留原始空间的全局结构，即所有点对的相对距离，用于去噪和线性分类预处理；后三者只保留局部结构，即局部聚类明显、全局结构可能失真，用于聚类可视化和非线性特征提取。我们首先采用了PCA降维，即标准化数据、求协方差矩阵、特征分解、选择k个主成分（特征向量）最后构造投影矩阵J进行降维。此后将尝试UMAP（Uniform Manifold Approximation and Projection）和t-SNE（计算复杂度较高）的方法。

其中**UMAP**是一种基于流形学习的非线性降维技术，它**假设数据均匀分布在一个潜在的流形上**，通过拓扑方法将高维数据映射到低维空间，同时尽可能保留数据的局部和全局结构。

首先，**构建高维空间的模糊拓扑结构**，通过**NN-Descent算法（**基于迭代**局部搜索**的近似K-NNG构建算法，核心思想是通过邻居的邻居逐步优化连接**）**得到KNN-G，根据权重函数计算边权重：（设k为k个邻居）

$\rho_i = min \lbrace d(x_i, x_{i_j} | 1 \le j \le k, d(x_i, x_j) > 0)\rbrace$

$\sum_{j=1}^{k}exp(\frac{-max(0,d(x_i,x_{i_j})-\rho_i)}{\sigma_i}) = log_2k.$  sigma is  归一化常量

$w(x_i, x_{i_j}) = exp(\frac{-max(0,d(x_i,x_{i_j})-\rho_i}{\sigma_i}).$ 

w表示点之间的连接强度；w越大说明距离越近。其次，使用随机初始化或PCA等线性方法**生成初始低维坐标**。接着通过**交叉熵**定义损失函数：高维和低维概率分布的差异。

$L = \sum_{i,j}p_{ij}log(\frac{p_{ij}}{q_{ij}}) + (1-p_{ij})log(\frac{1-p_{ij}}{1-q_{ij}}).$

通过梯度下降优化低维嵌入，使得低维空间的连接概率分布与高维空间的模糊拓扑结构尽可能一致。

**SNE**也是将点对距离转化成连接概率这一属性，在高维空间中，我们定义点$x_i$连接到x_j的条件概率为：
$p_{j|i} = \frac{exp(-||x_i - x_j||^2 / 2\sigma_i^2)}{\sum_{k \neq i}exp(-||x_i - x_k||^2 / 2\sigma_i^2)}$, 其中$\sigma_i$ 为点$x_i$中心的高斯分布方差  
在投影后的低维空间中，上述$\sigma$取$\frac{1}{\sqrt{2}}$
为了量化两个空间中数据分布的不一致性，我们定义代价函数C如下：

同样使用梯度下降最小化代价函数。
**SNE缺点**：使用高斯分布计算低维空间中的点之间的相似度，会导致近距离的高维点映射到低维时，容易发生“拥挤”现象，即低维空间中点过于密集，导致高维空间的全局结构难以展现。代价函数不对称，梯度下降较复杂
**t-SNE**: 采用SNE代价函数的对称版本，简化梯度计算；采用 Student-t distribution 而非高斯分布计算低维空间中的点对相似度，由于Student-t分布是重尾的，可以缓解高斯分布的拥挤现象

t-SNE存在几个问题：

1. 计算成本高
2. 如何选择自由度尚未有明确的研究
3. 没有唯一最优解，且不能用于预测，比如测试集合降维，因为他没有显式的预估部分，不能在测试集合直接降维

### 增量倒排索引优化

目前我们只是使用了普通的k-means算法实现的聚类索引。此后我们先将单独尝试自增式倒排索引Ada-IVF算法和经典的HNSW算法。
常规的IVF索引直接通过追加或删除分区来修改IVF索引，虽然能支持更新操作，但随着向量聚类状态偏离初始分布，此类操作会导致分区不均衡和重建误差的增加，进而降低搜索质量与性能。**LIRE**对违反预设大小阈值的分区进行分裂/合并，并通过质心距离选择相邻分区重新分配向量以最小化重建误差，虽然能减少分区不均衡，但维护开销随违规分区数量波动，未全局权衡效率与效果。为避免更新后索引质量的下降或全局重建索引带来的时间开销，Ada-IVF使用两个核心组件：1. **自适应维护策略**：动态识别存在性能问题的索引分区，并决定哪些分区需要重新划分；2. **本地重新聚类机制**：确定如何对选定分区进行重新划分。

**基本步骤**：

1. 维护索引元数据。Ada-IVF维护分区的大小、聚心、温度。温度表示读操作近期发生频率的高低。搜索操作会改变分区的温度，更新操作改变分区的内容、大小、聚心。
2. 检测分区的不平衡度和重建误差。重索引分数C是不平衡度和重建误差的函数。重索引分数超过阈值的分区被称为violators
3. 使用局部索引重索引violators。violators与它们的邻居分区D进行分裂和合并，最小化不平衡度和重建误差。接着检测全局偏差：分区大小标准差和分区的重建误差累积和

检查重建索引时：先逐个使用局部指示函数f检测是否存在local violations， 进行Local Reindex， 再用全局指示函数G检测是否存在Global violation， 执行全局重建。
$f(c,c_0) = \alpha c.T(\beta \frac{||s-s0||}{min{s,s0}} + (1-\beta)\frac{||\mu - \mu_0||}{||\mu_0||})$, 其中c为给定的分区，$c_0$为初始分区状态，考虑了分区温度、分区大小误差、分区均值偏移。
当f(c,c0)取值大于阈值$\tau_f$时，标记c为violators

**局部重索引算法**包含三个步骤

1. 分裂并删除violating分区(如果violators的大小超过预设定分区大小$\tau_s$，先使用k-means进行一次分裂)，找到离它最近的分区(根据聚心)
2. 对violating分区和其邻居分区中的向量使用k-means构建新的分区，并添加到剩余分区上。

如上图所示，$r_c$用于控制violators周围查找的邻居分区个数，越大可以减少重建误差（MSE），但是会增大重建时长。

**全局重索引** 使用全局指示函数G
$G(\epsilon, \sigma, \epsilon^{'}) = \gamma \frac{|\sigma - \sigma_0|}{\sigma_0} + (1-\gamma)G_d(\epsilon, \epsilon^{'}) \le \tau_G$, 其中$\sigma$是分区大小的标准偏差，$G_d$是全局重建误差函数，衡量当前重建误差与全局重建之后的重建误差的比值。

我们将尝试借鉴自适应的倒排索引Ada-IVF的思想解决SPFresh中数据漂移的问题：即当分区数据高频更新时或引入完全不同的特征数据时，LIRE的局部再平衡可能无法及时纠正累计误差。此时我们会通过定期统计个分区的数据分布（如均值、方差、类别比例），检测分布偏移，引入在线学习机制，动态调整分区策略，当检测到新聚类的出现时，自动创建新分区并更新导航边。
